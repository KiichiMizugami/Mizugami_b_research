from transformers import T5ForConditionalGeneration, T5Tokenizer
import torch
import time
import re

# ==========================================
# è¨­å®š
# ==========================================
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}\n")

# ãƒãƒƒãƒã‚µã‚¤ã‚ºã®è¨­å®š (GPUã®ãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦èª¿æ•´)
BATCH_SIZE = 16
# ğŸ’¡ T5-Smallã‚„T5-Baseãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®GECãƒ¢ãƒ‡ãƒ«ãªã®ã§ã€å¤§ãã‚ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒä½¿ãˆã¾ã™ã€‚

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿ï¼ˆå¤‰æ›´ãªã—ï¼‰
MODEL_NAME = "prithivida/grammar_error_correcter_v1"
start_time_load = time.time()
print("Loading model and tokenizer...")
tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)
model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)
print(f"Model loaded successfully in {time.time() - start_time_load:.2f} seconds.\n")

# ãƒ•ã‚¡ã‚¤ãƒ«å
INPUT_FILE = "essays_output_qwen25_7b.txt"
OUTPUT_FILE = "corrected_optimized.txt"

# ==========================================
# ã‚¹ãƒ†ãƒƒãƒ—1: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¨æ–‡å˜ä½ã¸ã®åˆ†å‰²
# ==========================================
# Qwenã®å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‹ã‚‰ã‚¨ãƒƒã‚»ã‚¤ã®æœ¬æ–‡ã ã‘ã‚’æŠ½å‡ºã—ã€ã•ã‚‰ã«æ–‡å˜ä½ã«åˆ†å‰²ã™ã‚‹é–¢æ•°
def extract_and_split_sentences(filepath):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¨ãƒƒã‚»ã‚¤ã‚’èª­ã¿è¾¼ã¿ã€ä¸è¦ãªãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤å»ã—ã€æ–‡å˜ä½ã§ãƒªã‚¹ãƒˆåŒ–ã™ã‚‹ã€‚
    """
    sentences_list = []
    
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()
    except FileNotFoundError:
        print(f"Error: Input file not found at {filepath}")
        return []

    # 1. <<< Essay X >>> ã¨ --- ã§æŒŸã¾ã‚ŒãŸã‚¨ãƒƒã‚»ã‚¤æœ¬æ–‡ã‚’æŠ½å‡º
    # re.DOTALL: . ãŒæ”¹è¡Œæ–‡å­—ã‚‚å«ã‚€ã‚ˆã†ã«ã™ã‚‹
    essay_texts = re.findall(r"<<< Essay \d+ >>>\n(.*?)\n---", content, re.DOTALL)
    
    # 2. ã‚¨ãƒƒã‚»ã‚¤æœ¬æ–‡ã‚’æ–‡å˜ä½ã«åˆ†å‰²ï¼ˆç°¡æ˜“çš„ãªå¥èª­ç‚¹åˆ†å‰²ï¼‰
    for text in essay_texts:
        # å¥ç‚¹(.)ã€ç–‘å•ç¬¦(?)ã€æ„Ÿå˜†ç¬¦(!)ã§åˆ†å‰²
        # re.splitã¯ãƒ‡ãƒªãƒŸã‚¿ã‚‚å«ã‚€ã‚ˆã†ã«ã‚«ãƒƒã‚³ã§å›²ã‚“ã§ã„ã¾ã™
        text = text.replace('\n', ' ') # æ”¹è¡Œã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«ç½®æ›
        s_list = re.split(r'([.?!])\s*', text.strip())
        
        # åˆ†å‰²çµæœã®çµåˆã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        sentence = ""
        for item in s_list:
            if item in ['.', '?', '!']:
                # å¥èª­ç‚¹ã‚’ç›´å‰ã®æ–‡ã«çµåˆã—ã€æ–‡ãƒªã‚¹ãƒˆã«è¿½åŠ 
                sentences_list.append((sentence + item).strip())
                sentence = "" # sentenceã‚’ãƒªã‚»ãƒƒãƒˆ
            else:
                sentence += item
        if sentence.strip():
             # æœ€å¾Œã«æ®‹ã£ãŸæ–‡ã‚’è¿½åŠ 
            sentences_list.append(sentence.strip())

    # éå¸¸ã«çŸ­ã„æ–‡ã‚„ç©ºã®æ–‡ã‚’é™¤å»
    return [s for s in sentences_list if len(s.split()) >= 3]

all_sentences = extract_and_split_sentences(INPUT_FILE)
total_sentences = len(all_sentences)

if total_sentences == 0:
    print("No valid sentences extracted. Exiting.")
    # ãƒ¢ãƒ‡ãƒ«ã‚’å‰Šé™¤ã—ã¦ãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾
    del model
    torch.cuda.empty_cache()
    # å¿…è¦ã«å¿œã˜ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¦çµ‚äº†
    with open(OUTPUT_FILE, "w") as f:
        f.write("No sentences processed.")
    
# ==========================================
# ã‚¹ãƒ†ãƒƒãƒ—2: ãƒãƒƒãƒå‡¦ç†ã¨è¨‚æ­£ã®å®Ÿè¡Œ
# ==========================================
print(f"Total sentences extracted and ready for correction: {total_sentences}")
start_time_gen = time.time()
corrected_sentences = []

for i in range(0, total_sentences, BATCH_SIZE):
    # ãƒãƒƒãƒã‚’å–å¾—
    batch = all_sentences[i:i + BATCH_SIZE]
    
    # GECã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä»˜ä¸
    input_texts = ["gec: " + s for s in batch]
    
    # ãƒãƒƒãƒãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    input_ids = tokenizer(input_texts, return_tensors='pt', padding=True, truncation=True).to(device)
    
    # ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰
    outputs = model.generate(
        **input_ids,
        max_length=64,
        num_beams=4,
        early_stopping=True
    )
    
    # ãƒ‡ã‚³ãƒ¼ãƒ‰
    batch_corrected = [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]
    corrected_sentences.extend(batch_corrected)
    
    # é€²æ—è¡¨ç¤º
    print(f"Processed {i + len(batch)} / {total_sentences} sentences...")

total_time = time.time() - start_time_gen
print(f"\nCorrection completed. Total sentences: {total_sentences}")
print(f"Total correction time: {total_time:.2f} seconds.")
print(f"Average time per sentence: {total_time / total_sentences:.4f} seconds.")

# ==========================================
# ã‚¹ãƒ†ãƒƒãƒ—3: è¨‚æ­£çµæœã®å‡ºåŠ›
# ==========================================
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    for original, corrected in zip(all_sentences, corrected_sentences):
        f.write(f"Original: {original}\n")
        f.write(f"Corrected: {corrected}\n")
        f.write("-" * 50 + "\n")

print(f"\nã™ã¹ã¦ã®è¨‚æ­£çµæœã‚’ '{OUTPUT_FILE}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

# ãƒ¢ãƒ‡ãƒ«ã‚’å‰Šé™¤ã—ã¦ãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾
del model
torch.cuda.empty_cache()